# Data

The dataset we use is called modeled exposure to particulate matter air pollution, that aggregates the concentration of the fine particulate matter (PM 2.5) in 194 over the world. In particular, it includes the annual concentration of PM 2.5 in 194 countries, with breakdown for urban and rural areas. The dataset is authored by the World Health Organization (WHO). In general, to obtain estimates of PM 2.5 levels, the concentration of PM 2.5 is measured from fixed-site monitors located within metropolitan areas. The value of PM 2.5 in a given area can be obtained by averaging the measurements of monitors in this area.

Our dataset includes the average annual PM 2.5 concentration on 194 states from 2010 to 2019.  Apart from the average value over the country, the dataset also records the averages on four different area types: urban, rural, cities and towns. The dataset consists of 34 variables in total. Not all of them will be valuable to us, but relevant features include country, location (continent), period (year), area type (detailed below), and value (PM 2.5 concentration). There are 9450 observations, each recording the average annual concentration of PM 2.5 for a specific area type (urban, rural, cities, towns, total) of one of the 194 states in the world.

We obtain the data from the WHO official website. A CSV file can be downloaded from this link. The file is ready-to-use, requiring only a “read.csv” function to be loaded into RStudio. We will perform data cleaning (eliminating redundant variables and dealing with NAs) and analysis using this file.


## Description

The data provides us comprehensive information about the mean, minimum, and max PM2.5 at each residence area type, country, continent from 2010 to 2019. Here are some ways that I’m going to use to analyze the data based on our needs.

1. Pre-processing:
    a. Remove unneeded value and handle NaN value.
    b. Transform the data for later use by applying pivot_longer(), mutate(), etc.
    
2. Analyze air quality change over time:
    a. Use time series line chart to plot PM2.5 change from 2010 to 2019, smooth the plot to see the general trend. This can be done in geom-line() function.
    b. Create multiple box charts that shows min, max, mean value for each year and each continent in facet graph using facet_wrap().
    c. Create a histogram that shows the average PM2.5 for each country and continent.
    
3. Analyze air quality change over location:
    a. Create a Heatmap using geom_tile to show country differences for PM2.5 and highlight those outliers.
    b. Use bar chart to show the average and range of PM2.5 per continent and country.
    
4. Analyze air quality change over resident type (urban, rural, cities, towns):
    a. Visualize the resident type difference per continent by evaluating the average PM2.5 in the facet graph using ggplot and facet_wrap().
    b. Plot the average PM2.5 per continent per resident type as a scatter plot to show if the PM2.5 are significantly different across all resident types.
    c. Create a ggalluvial plot over each resident type and year to show the value change for each resident type.
    
5. Putting it all together, there are 3 dimensions that we are going to analyze the data, at the end, based on what we observe from the visualizations above, we will use any package that we learned from class to dive deeper into the detail to analyze the abnormality or general trends to validate our hypothesis and make the analysis comprehensive.


## Missing value analysis

Let’s look at our data and understand it. We only have 1 dataset with 34 columns and 9450 rows. We first check if there are any columns that have no useful information for our analysis and select the ones that are useful for us.

The second step is to check if the remaining columns have any missing or incorrect values and adjust them. Here are the breakdown steps:

```{r}
# import
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)

# read csv
data <- read_csv("data_pollution.csv")

```
```{r}

# count distinct values
distinct_counts <- data %>% summarise_all(n_distinct)

long_data <- pivot_longer(distinct_counts, cols = everything(), names_to = "ColumnName", values_to = "DistinctCount")

# plot the number of distinct values per column in our dataset
ggplot(long_data, aes(x = ColumnName, y = DistinctCount)) +
  geom_bar(stat = "identity") +
  labs(x = "Column Name", y = "# of distinct values", title = "Check distinct values")

```

We can see a lot of columns have a few distinct value, let's check if they equals 1 or 0, if yes, then we may need to remove these columns since they don't contribute to our analysis.

```{r}

# check insignificant data
insignificant_data <- long_data %>% 
                               filter(DistinctCount <= 1)
insignificant_data
```

After double-checking our dataset, we found these data could be removed. We then check the NaN values of the rest of the columns.

```{r}

# check for NaN
selected_columns <- setdiff(colnames(data), colnames(insignificant_data))
selected_data <- data %>% select(all_of(selected_columns))

nan_counts <- sapply(selected_data, function(x) sum(is.nan(x)))

nan_counts
```

We can conclude that there is no missing value in the data we selected. 
